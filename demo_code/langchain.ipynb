{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phi2():\n",
    "    model_id = \".../echo_chamber/original_phi2\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=100)\n",
    "    hf = HuggingFacePipeline(pipeline=pipe)\n",
    "    return hf\n",
    "\n",
    "langchain_llm  = ChatOpenAI(\n",
    "\ttemperature=0,\n",
    "    openai_api_key = '...',\n",
    "\tmodel_name=\"gpt-3.5-turbo\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(llm=get_phi2())\n",
    "print(conversation.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.conversation.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "conversation = ConversationChain(\n",
    "\tllm=get_phi2(),\n",
    "\tmemory=ConversationSummaryBufferMemory(llm=langchain_llm, max_token_limit=300)\n",
    ")\n",
    "\n",
    "conversation(\"Good morning AI!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conversation.memory.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation(\"Tell me why vaccine would kill us.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 'history': \"\\nThe human asks the AI about the history of the United States. The AI tells the human about the Civil War, which was fought between the Northern and Southern states over the issue of slavery.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n\\n\\nNew lines of conversation:\\nHuman: Hi AI!\\nAI: Hello Human! How can I assist you today?\\nHuman: I'm trying to learn more about the history of the United States. Can you tell me about the Civil\",\n",
    "\n",
    " 'history': 'The human greets the AI and asks for information about the Civil War. The AI explains that the Civil War was a major conflict fought between the Northern Union states and the Southern Confederate states from 1861 to 1865, primarily over the issue of slavery.',\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
